<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: planet postgresql | Peter Eisentraut]]></title>
  <link href="http://peter.eisentraut.org/blog/categories/planet-postgresql/atom.xml" rel="self"/>
  <link href="http://peter.eisentraut.org/"/>
  <updated>2015-08-15T09:52:09-04:00</updated>
  <id>http://peter.eisentraut.org/</id>
  <author>
    <name><![CDATA[Peter Eisentraut]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Have problems with PostgreSQL?  Try using Hive!]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/08/14/have-problems-with-postgresql-try-using-hive"/>
    <updated>2015-08-14T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/08/14/have-problems-with-postgresql-try-using-hive</id>
    <content type="html"><![CDATA[<p>So I had this PostgreSQL database that was getting a bit too big, and
since it was really only for analytics, I figured it would be a good
fit for putting in Hadoop+Hive instead.</p>

<p>(For those not completely familiar with this: Hadoop is sort of a job
tracker and distributed file system. Hive is an SQL-like layer on top
of that.  I know the cool kids are now using Spark.  Maybe for another
day.)</p>

<p>The first thing you need to learn about the Hadoop ecosystem is its
idiosyncratically fragmented structure.  With PostgreSQL, you
basically have the community website, the community mailing lists, the
community source code distribution, the community binaries, and a
handful of binaries made by Linux distributions.  If you search the
web for a problem with PostgreSQL, you will normally gets hits on one
or more of: the documentation, the mailing lists, third-party mirrors
of the mailing lists, or Stack Overflow.  With Hadoop, you have the
resources provided by the Apache Software Foundation, including the
source distribution, bug tracker, documentation, and then bunch of
commercial vendors with their parallel universes, including their own
mutually incompatible binary distributions, their own copy of the
documentation, their own mailing lists, their own bug trackers, etc.
When you search for a problem with Hadoop, you will typically get hits
from three separate copies of the documentation, about eight mailing
lists, fifteen tutorials, and one thousand blog posts.  And about 20
unanswered posts on Stack Overflow.  Different vendors also favor
different technology extensions.  So if, say, you read that you should
use some storage method, chances are it&rsquo;s not even supported in a
given distribution.</p>

<p>The next thing to know is that any information about Hadoop that is
older than about two years is obsolete.  Because they keep changing
everything from command names to basic architecture.  Don&rsquo;t even
bother reading old stuff.  Don&rsquo;t even bother reading anything.</p>

<p>So Hive.  The basic setup is actually fairly well documented.  You set
up a Hadoop cluster, HDFS, create a few directories.  Getting the
permissions sorted out during these initial steps is not easy, but it
seldom is.  So you can create a few tables, load some data, run a few
queries.</p>

<p>Nevermind that in its default configuration <code>hive</code> spits out about a
dozen warnings on every startup about deprecated parameters and jar
file conflicts.  This is apparently well known.  Look around in the
internet for hive examples.  They show the same output.  Apparently
the packaged versions of Hadoop and Hive are not tuned for each other.</p>

<p>Then you learn: In the default configuration, there can only be <em>one</em>
Hive session connected at once.  It doesn&rsquo;t tell you this.  Instead,
when the second session wants to connect, it tells you</p>

<pre><code>Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
</code></pre>

<p>followed by hundreds of lines of exception traces.  This is Hive-speak
for: &ldquo;there is already one session connected&rdquo;.</p>

<p>You see, Hive needs a, cough, cough, relational database to store its
schema.  By default, it uses embedded Derby, which allows only one
connection at a time.  If you want to connect more than one session at
once, you need to set up an external &ldquo;Hive metastore&rdquo; on a MySQL or
PostgreSQL database.</p>

<p>Nevermind that Derby can actually run in server mode.  That&rsquo;s
apparently not supported by Hive.</p>

<p>So I had a PostgreSQL database handy and tried to set that up.  I
installed the PostgreSQL JDBC driver, created an external database,
changed the Hive configuration to use an external database.</p>

<p>At this point, it turned out that the PostgreSQL JDBC driver was
broken, so I had to downgrade to an older version.  (The driver has
since been fixed.)</p>

<p>After I got one that was working, Hive kept complaining that it
couldn&rsquo;t find a driver that matches the JDBC URL
<code>jdbc:postgresql://somehost/hive_metastore</code>.  The PostgreSQL JDBC
driver explains in detail how to load the driver, but how do I get
that into Hive?</p>

<p>The first suggestion from the internet was to add something like this
to <code>.hiverc</code>:</p>

<pre><code>add jar /usr/share/java/postgresql-jdbc.jar;
</code></pre>

<p>That doesn&rsquo;t work.  Remember, don&rsquo;t believe anything you read on the
internet.</p>

<p>In between I even tried download the MySQL JDBC driver (no, I don&rsquo;t
want to sign in with my Oracle account), but it had the same problem.</p>

<p><code>hive</code> is actually a shell script which loads another shell script
which loads a bunch of other shell scripts, which eventually starts
<code>java</code>.  After randomly poking around I determined that if I did</p>

<pre><code>export HIVE_AUX_JARS_PATH=/usr/share/java/
</code></pre>

<p>it would pick up the jar files in that directory.  OK, that worked.</p>

<p>Now I can create tables, load data, run simple queries, from more than
one session.  So I could do</p>

<pre><code>SELECT * FROM mytable;
</code></pre>

<p>But as soon as I ran</p>

<pre><code>SELECT count(*) FROM mytable;
</code></pre>

<p>it crapped out again:</p>

<pre><code>java.io.FileNotFoundException: File does not exist: hdfs://namenode/usr/share/java/jline-0.9.94.jar
</code></pre>

<p>So it&rsquo;s apparently looking for some jar file on HDFS rather than the
regular file system.  Some totally unrelated jar file, too.</p>

<p>The difference between the two queries is that the first one is
answered by just dumping out data locally, whereas the second one
generates a distributed map-reduce job.  It doesn&rsquo;t tell you that
beforehand, of course.  Or even afterwards.</p>

<p>After a while I figured that this must have something to do with the
<code>HIVE_AUX_JARS_PATH</code> setting.  I changed that to</p>

<pre><code>export HIVE_AUX_JARS_PATH=/usr/share/java/postgresql-jdbc.jar;
</code></pre>

<p>so it would look at only one file, and sure enough it now complains</p>

<pre><code>java.io.FileNotFoundException: File does not exist: hdfs://namenode/usr/share/java/postgresql-jdbc.jar
</code></pre>

<p>Apparently, the <code>HIVE_AUX_JARS_PATH</code> facility is for adding jars that
contain user-defined functions that you need at run time.  As far as I
can tell, there is no separate setting for adding jars that you only
need locally.</p>

<p>There are workarounds for that on the internet, of varying
bizarreness, none of which worked.  Remember, don&rsquo;t believe anything
you read on the internet.</p>

<p>In the end, I indulged it and just uploaded that jar file into HDFS.
Whatever.</p>

<p>I then put my data loading job into cron, which quickly crapped out
because <code>JAVA_HOME</code> is not set in the cron environment.  After that
was fixed, I let my data loading jobs run for a while.</p>

<p>Later, I wanted clear out the previous experiments, drop all tables,
and start again.  Apparently, dropping a table in Hive takes a very
long time.  Actually, no.  When you use PostgreSQL for the Hive
metastore, any attempt to drop a table with <em>hang indefinitely</em>.</p>

<p><a href="https://www.mail-archive.com/user@hive.apache.org/msg00515.html">Someone</a>
summarized the issue:</p>

<blockquote><p>You are the first person I have heard of using postgres. I commend
you for not succumbing to the social pressure and just installing
mysql.  However I would advice succumbing to the social pressure and
using either derby or mysql.</p>

<p>The reason I say this is because jpox
&ldquo;has support&rdquo; for a number of data stores (M$ SQL server) however,
people have run into issues with them. Databases other then derby
and mysql &lsquo;should work&rsquo; but are generally untested.</p></blockquote>

<p>Not that actually testing it would take much work.  It&rsquo;s not like Hive
doesn&rsquo;t have any tests.  Just add some tests.</p>

<p>It&rsquo;s funny that they didn&rsquo;t write &ldquo;You are the first person I have
heard of using hive&rdquo;.  Clearly, nobody has ever actually used this.</p>

<p>Anyway, somehow I ended up creating the metastore schema manually by
copying and pasting various pieces from the internet and raw files.
Shudder.</p>

<p>How about more fun?  Here is a run-of-the-mill SQL parse error:</p>

<pre><code>NoViableAltException(26@[221:1: constant : ( Number | dateLiteral | StringLiteral | stringLiteralSequence | BigintLiteral | SmallintLiteral | TinyintLiteral | DecimalLiteral | charSetStringLiteral | booleanValue );])
    at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
    at org.antlr.runtime.DFA.predict(DFA.java:116)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.constant(HiveParser_IdentifiersParser.java:4377)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.partitionVal(HiveParser_IdentifiersParser.java:8444)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.partitionSpec(HiveParser_IdentifiersParser.java:8283)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.tableOrPartition(HiveParser_IdentifiersParser.java:8161)
    at org.apache.hadoop.hive.ql.parse.HiveParser.tableOrPartition(HiveParser.java:31397)
    at org.apache.hadoop.hive.ql.parse.HiveParser.insertClause(HiveParser.java:30914)
    at org.apache.hadoop.hive.ql.parse.HiveParser.regular_body(HiveParser.java:29076)
    at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatement(HiveParser.java:28968)
    at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:28762)
    at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1238)
    at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:938)
    at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:190)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1000)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)
    at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)
    at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:622)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
FAILED: ParseException line 4:20 cannot recognize input near 'year' '(' 'event_timestamp' in constant
</code></pre>

<p>In PostgreSQL, this might say</p>

<pre><code>syntax error at or near "("
</code></pre>

<p>with a pointer to the actual query.</p>

<p>I just put a function call somewhere where it didn&rsquo;t belong.  The
documentation is very terse and confusing about a lot of these things.
And the documentation is kept as a series of wiki pages.</p>

<p>So now I have a really slow distributed version of my PostgreSQL
database, which stores its schema in another PostgreSQL database.
I forgot why I needed that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Storing URIs in PostgreSQL]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/04/16/storing-uris-in-postgresql"/>
    <updated>2015-04-16T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/04/16/storing-uris-in-postgresql</id>
    <content type="html"><![CDATA[<p>About two months ago, this happened:</p>

<blockquote class="twitter-tweet" lang="en"><p>What form of bribery would be required to convince someone to write an `email` and `url` data type for Postgres, by the way? Any takers?</p>&mdash; Peter van Hardenberg (@pvh) <a href="https://twitter.com/pvh/status/567395527357001728">February 16, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>And a few hours later:</p>

<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" lang="en"><p><a href="https://twitter.com/pvh">@pvh</a> <a href="https://t.co/NNxHRsHudz">https://t.co/NNxHRsHudz</a></p>&mdash; Peter Eisentraut (@petereisentraut) <a href="https://twitter.com/petereisentraut/status/567902299194073089">February 18, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>It took a few more hours and days after this to refine some details,
but I have now tagged the first
<a href="https://github.com/petere/pguri/releases/tag/1.20150415">release</a> of
this extension.  Give it a try and let me know what you think.  Bug
reports and feature requests are welcome.</p>

<p>(I chose to name the data type <code>uri</code> instead of <code>url</code>, as originally
suggested, because that is more correct and matches what the parsing
library calls it.  One could create a domain if one prefers the other
name or if one wants to restrict the values to certain kinds of URIs
or URLs.)</p>

<p>(If you are interested in storing email addresses,
<a href="https://wiki.postgresql.org/wiki/Email_address_parsing">here</a> is an
idea.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Retrieving PgBouncer statistics via dblink]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/03/25/retrieving-pgbouncer-statistics-via-dblink"/>
    <updated>2015-03-25T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/03/25/retrieving-pgbouncer-statistics-via-dblink</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.postgresql.org/wiki/PgBouncer">PgBouncer</a> has a virtual
database called <code>pgbouncer</code>.  If you connect to that you can run
special SQL-like commands, for example</p>

<pre><code>$ psql -p 6432 pgbouncer
=# SHOW pools;
┌─[ RECORD 1 ]───────────┐
│ database   │ pgbouncer │
│ user       │ pgbouncer │
│ cl_active  │ 1         │
│ cl_waiting │ 0         │
│ sv_active  │ 0         │
│ sv_idle    │ 0         │
│ sv_used    │ 0         │
│ sv_tested  │ 0         │
│ sv_login   │ 0         │
│ maxwait    │ 0         │
└────────────┴───────────┘
</code></pre>

<p>This is quite nice, but unfortunately, you cannot run full SQL queries
against that data.  So you couldn&rsquo;t do something like</p>

<pre><code class="sql">SELECT * FROM pgbouncer.pools WHERE maxwait &gt; 0;
</code></pre>

<p>Well, here is a way: From a regular PostgreSQL database, connect to
PgBouncer using dblink.  For each <code>SHOW</code> command provided by
PgBouncer, create a view.  Then that SQL query actually works.</p>

<p>But before you start doing that, I have already done that here:</p>

<p><div><script src='https://gist.github.com/b4e2aa7cb4a073e07630.js?file=pgbouncer-schema.sql'></script>
<noscript><pre><code>CREATE EXTENSION dblink;


-- customize start
CREATE SERVER pgbouncer FOREIGN DATA WRAPPER dblink_fdw OPTIONS (host &#39;localhost&#39;,
                                                                 port &#39;6432&#39;,
                                                                 dbname &#39;pgbouncer&#39;);

CREATE USER MAPPING FOR PUBLIC SERVER pgbouncer OPTIONS (user &#39;pgbouncer&#39;);
-- customize stop


CREATE SCHEMA pgbouncer;

CREATE VIEW pgbouncer.clients AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show clients&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text);

CREATE VIEW pgbouncer.config AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show config&#39;) AS _(key text, value text, changeable boolean);

CREATE VIEW pgbouncer.databases AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show databases&#39;) AS _(name text, host text, port int, database text, force_user text, pool_size int, reserve_pool int);

CREATE VIEW pgbouncer.lists AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show lists&#39;) AS _(list text, items int);

CREATE VIEW pgbouncer.pools AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show pools&#39;) AS _(database text, &quot;user&quot; text, cl_active int, cl_waiting int, sv_active int, sv_idle int, sv_used int, sv_tested int, sv_login int, maxwait int);

CREATE VIEW pgbouncer.servers AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show servers&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text);

CREATE VIEW pgbouncer.sockets AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show sockets&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text,
                                                           recv_pos int, pkt_pos int, pkt_remain int, send_pos int, send_remain int, pkt_avail int, send_avail int);
</code></pre></noscript></div>
</p>

<p>Here is another useful example.  If you&rsquo;re tracing back connections
from the database server through PgBouncer to the client, try this:</p>

<pre><code class="sql">SELECT * FROM pgbouncer.servers LEFT JOIN pgbouncer.clients ON servers.link = clients.ptr;
</code></pre>

<p>Unfortunately, different versions of PgBouncer return a different
number of columns for some commands.  Then you will need different
view definitions.  I haven&rsquo;t determined a way to handle that
elegantly.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The history of replication in PostgreSQL]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/03/03/the-history-of-replication-in-postgresql"/>
    <updated>2015-03-03T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/03/03/the-history-of-replication-in-postgresql</id>
    <content type="html"><![CDATA[<h2>2001: PostgreSQL 7.1: write-ahead log</h2>

<p>PostgreSQL 7.1 introduced the write-ahead log (WAL).  Before that
release, all open data files had to be fsynced on every commit, which
is very slow.  Slow fsyncing is still a problem today, but now we&rsquo;re
only worried about fsyncing the WAL, and fsyncing the data files
during the checkpoint process.  Back then, we had to fsync
<em>everything</em> <em>all the time</em>.</p>

<p>In the original design of university POSTGRES, the lack of a log was
intentional, and contrasted with heavily log-based architectures such
as Oracle.  In Oracle, you need the log to roll back changes.  In
PostgreSQL, the nonoverwriting storage system takes care of that.  But
probably nobody thought about implications for fsyncing back then.</p>

<p>Note that the WAL was really just an implementation detail at this
point.  You couldn&rsquo;t read or archive it.</p>

<h2>2004: Slony</h2>

<p>Just for context: Slony-I 1.0 was
<a href="http://lists.slony.info/pipermail/slony1-general/2004-July/000106.html">released</a>
in July 2004.</p>

<h2>2005: PostgreSQL 8.0: point-in-time recovery</h2>

<p>PostgreSQL 8.0 added the possibility to copy the WAL somewhere else,
and later play it back, either all the way or to a particular point in
time, hence the name point-in-time recovery (PITR) for this feature.
This feature was mainly intended to relieve <code>pg_dump</code> as a backup
method.  Until then, the only backup method was a full dump, which
would get impractical as databases grew.  Hence this method to take an
occasional base backup, which is the expensive part, and then add on
parts of the WAL, which is cheaper.</p>

<p>The basic configuration mechanisms that we still use today, for
example the <code>recovery.conf</code> file, were introduced as part of this
feature.</p>

<p>But still no replication here.</p>

<h2>2008: PostgreSQL 8.3: pg_standby</h2>

<p>Crafty people eventually figured that if you archived WAL on one
server and at the same time &ldquo;recovered&rdquo; endlessly on another, you&rsquo;d
have a replication setup.  You could probably have set this up with
your own scripts as early as 8.0, but PostgreSQL 8.3 added the
<code>pg_standby</code> program into <code>contrib</code>, which gave everyone a standard
tool.  So, arguably, 8.3 is the first release that contained a
semblance of a built-in replication solution.</p>

<p>The standby server was in permanent recovery until promotion, so it
couldn&rsquo;t be read from as it was replicating.  This is what we&rsquo;d now
call a warm standby.</p>

<p>I think a lot of PostgreSQL 8.3 installations refuse to die, because
this is the first version where you could easily have a reasonably
up-to-date reserve server without resorting to complicated and
sometimes problematic tools like Slony or DRBD.</p>

<h2>2010: PostgreSQL 9.0: hot standby, streaming replication</h2>

<p>In PostgreSQL 9.0, two important replication features arrived
completely independently.  First, the possibility to connect to a
standby server in read-only mode, making it a so-called hot standby.
Whereas before, a standby server was really mainly useful only as a
reserve in case the primary server failed, with hot standby you could
use secondary servers to spread out read-only loads. Second, instead
of relying solely on the WAL archive and recovery functionalities to
transport WAL data, a standby server could connect directly to the
primary server via the existing libpq protocol and obtain WAL data
that way, so-called streaming replication.  The primary use in this
release was that the standby could be more up to date, possibly within
seconds, rather than several minutes with the archive-based approach.
For a robust setup, you would still need to set up an archive.  But
streaming replication was also a forward-looking feature that would
eventually make replication setups easier, by reducing the reliance on
the old archiving mechanisms.</p>

<p>PostgreSQL 9.0 was the first release where one could claim that
PostgreSQL &ldquo;supports replication&rdquo; without having to make
qualifications or excuses.  Although it is scheduled to go EOL later
this year, I expect this release will continue to live for a long
time.</p>

<h2>2011: PostgreSQL 9.1: pg_basebackup, synchronous replication</h2>

<p><code>pg_basebackup</code> was one of the features facilitated by streaming
replication that made things easier.  Instead of having to use
external tools like <code>rsync</code> for base backups, <code>pg_basebackup</code> would
use a normal libpq connection to pull down a base backup, thus
avoiding complicated connection and authentication setups for external
tools.  (Some people continue to favor <code>rsync</code> because it is faster
for them.)</p>

<p>PostgreSQL 9.1 also added synchronous replication, which ensures that
data is replicated to the designated synchronous standby before a
<code>COMMIT</code> reports success.  This feature is frequently misunderstood by
users.  While it ensures that your data is on at least two servers at
all times, it might actually reduce the availability of your system,
because if the standby server goes down, the primary will also go
down, unless you have a third server available to take over the
synchronous standby duty.</p>

<p>Less widely know perhaps is that PostgreSQL 9.1 also added the
<code>pg_last_xact_replay_timestamp</code> function for easy monitoring of
standby lag.</p>

<p>In my experience, the availability of <code>pg_basebackup</code> and
<code>pg_last_xact_replay_timestamp</code> make PostgreSQL 9.1 the first release
were managing replication was reasonably easy.  Go back further, and
you might feel constrained by the available tools.  But in 9.1, it&rsquo;s
not that much different from what is available in the most recent
releases.</p>

<h2>2012: PostgreSQL 9.2: cascading replication</h2>

<p>Not as widely acclaimed, more for the Slony buffs perhaps,
PostgreSQL 9.2 allowed standbys to fetch their streaming replication
data from other standbys.  A particular consequence of that is that
<code>pg_basebackup</code> could copy from a standby server, thus taking the load
off the primary server for setting up a new standby or standalone
copy.</p>

<h2>2013: PostgreSQL 9.3: standby can follow timeline switch</h2>

<p>This did not even make it into the release note highlights.  In
PostgreSQL 9.3, when a primary has two standbys, and one of the
standbys is promoted, the other standby can just keep following the
new primary.  In previous releases, the second standby would have to
be rebuilt.  This improvement makes dynamic infrastructure changes
much simpler.  Not only does it eliminate the time, annoyance, and
performance impact of setting up a new standby, more importantly it
avoids the situation that after a promotion, you don&rsquo;t have any up to
update standbys at all for a while.</p>

<h2>2014: PostgreSQL 9.4: replication slots, logical decoding</h2>

<p>Logical decoding got all the press for PostgreSQL 9.4, but I think
replication slots are the major feature, possibly the biggest
replication feature since PostgreSQL 9.0.  Note that while streaming
replication has gotten more sophisticated over the years, you still
needed a WAL archive for complete robustness.  That is because the
primary server didn&rsquo;t actually keep a list of its supposed standby
servers, it just streamed whatever WAL happened to be requested if it
happened to have it.  If the standby server fell behind sufficiently
far, streaming replication would fail, and recovery from the archive
would kick in.  If you didn&rsquo;t have an archive, the standby would then
no longer be able to catch up and would have to be rebuilt.  And this
archiving mechanism has essentially been unchanged since version 8.0,
when it was designed for an entirely different purpose.  So a
replication setup is actually quite messy: You have to configure an
access path from the primary to the standby (for archiving) and an
access path from the standby to the primary (for streaming).  And if
you wanted to do multiple standbys or cascading, maintaining the
archive could get really complicated.  Moreover, I think a lot of
archiving setups have problematic <code>archive_command</code> settings.  For
example, does your <code>archive_command</code> fsync the file on the receiving
side?  Probably not.</p>

<p>No more: In PostgreSQL 9.4, you can set up so-called replication
slots, which effectively means that you register a standby with the
primary, and the primary keeps around the WAL for each standby until
the standby has fetched it.  With this, you can completely get rid of
the archiving, unless you need it as a backup.</p>

<h2>2015? PostgreSQL 9.5? pg_rewind?</h2>

<p>One of the remaining problems is that promoting a standby leaves the
old primary unable to change course and follow the new primary.  If
you fail over because the old primary died, then that&rsquo;s not an issue.
But if you just want to swap primary and standby, perhaps because the
standby has more powerful hardware, then the old primary, now standby,
needs to be rebuilt completely from scratch.  Transforming an old
primary into a new standby without a completely new base backup is a
rather intricate problem, but a tool that can do it (currently named
<code>pg_rewind</code>) is proposed for inclusion into the next PostgreSQL
release.</p>

<h2>Beyond</h2>

<p>One of the problems that this evolution of replication has created is
that the configuration is rather idiosyncratic, quite complicated to
get right, and almost impossible to generalize sufficiently for
documentation, tutorials, and so on.  Dropping archiving with 9.4
might address some of these points, but configuring even just
streaming replication is still weird, even weirder if you don&rsquo;t know
how it got here.  You need to change several obscure configuration
parameters, some on the primary, some on the standby, some of which
require a hard restart of the primary server.  And then you need to
create a new configuration file <code>recovery.conf</code>, even though you don&rsquo;t
want to recover anything.  Making changes in this area is mostly a
complex political process, because the existing system has served
people well over many years, and coming up with a new system that is
obviously better and addresses all existing use cases is cumbersome.</p>

<p>Another issue is that all of this functionality has been bolted on to
the write-ahead log mechanism, and that constrains all the uses of the
write-ahead log in various ways.  For example, there are optimizations
that skip WAL logging in certain circumstances, but if you want
replication, you can&rsquo;t use them.  Who doesn&rsquo;t want replication?  Also,
the write-ahead log covers an entire database system and is all or
nothing.  You can&rsquo;t replicate only certain tables, for example, or
consolidate logs from two different sources.</p>

<p>How about not bolting all of this on to the WAL?  Have two different
logs for two different purposes.  This was discussed, especially
around the time streaming replication was built.  But then you&rsquo;d need
two logs that are <em>almost</em> the same.  And the WAL is by design a
bottleneck, so creating another log would probably create performance
problems.</p>

<p>Logical decoding breaks many of these restrictions and will likely be
the foundation for the next round of major replication features.
Examples include partial replication and multimaster replication, some
of which are being worked on right now.</p>

<p>What can we expect from plain WAL logging in the meantime?  Easier
configuration is certainly a common request.  But can we expect major
leaps on functionality?  Who knows.  At one point, something like hot
standby was thought to be nearly impossible.  So there might be
surprises still.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ccache and clang, part 3]]></title>
    <link href="http://peter.eisentraut.org/blog/2014/12/01/ccache-and-clang-part-3"/>
    <updated>2014-12-01T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2014/12/01/ccache-and-clang-part-3</id>
    <content type="html"><![CDATA[<p>In
<a href="http://petereisentraut.blogspot.com/2011/05/ccache-and-clang.html">part 1</a>
and
<a href="http://petereisentraut.blogspot.com/2011/09/ccache-and-clang-part-2.html">part 2</a>
I investigated how to use <code>ccache</code> with <code>clang</code>.  That was more than
three years ago.</p>

<p>I got an email the other day that
<a href="https://bugzilla.samba.org/show_bug.cgi?id=8118">ccache bug 8118</a>,
which I filed while writing part 1, was closed, as ccache 3.2 was
released.  The
<a href="https://ccache.samba.org/releasenotes.html#_ccache_3_2">release notes of clang 3.2</a>
contain several items related to clang.  So it was time to give this
another look.</p>

<p>Basically, the conclusions from part 2 still stand: You cannot use
<code>ccache</code> with <code>clang</code> without using <code>CCACHE_CPP2</code>.  And it is now
becoming clear to me that this is an issue that is not going to go
away, and it&rsquo;s not really even Clang&rsquo;s fault.</p>

<h2>Warnings!</h2>

<p>The problem is that <code>clang</code>&rsquo;s <code>-Wall</code> can cause warnings when
compiling the <em>preprocessed</em> version of otherwise harmless C code.
This can be illustrated by this piece of C code:</p>

<pre><code class="c">int
foo()
{
        int *p, *q;

        p = q = 0;
        p = p;
        if (p == p)
                return 1;
        if ((p == q))
                return 2;
        return 0;
}
</code></pre>

<p>When compiled by <code>gcc-4.9 -Wall</code>, this gives no warnings.  When
compiled by <code>clang-3.5 -Wall</code>, this results in
<code>
test.c:7:4: warning: explicitly assigning value of variable of type 'int *' to itself [-Wself-assign]
test.c:8:8: warning: self-comparison always evaluates to true [-Wtautological-compare]
test.c:10:9: warning: equality comparison with extraneous parentheses [-Wparentheses-equality]
test.c:10:9: note: remove extraneous parentheses around the comparison to silence this warning
test1.c:10:9: note: use '=' to turn this equality comparison into an assignment
</code></p>

<p>You wouldn&rsquo;t normally write code like this, but the C preprocessor
could create code with self-assignments, self-comparisons, extra
parentheses, and so on.</p>

<p>This example represents the issues I saw when trying to compile
PostgreSQL 9.4 with <code>ccache</code> and <code>clang</code>; there might be others.</p>

<p>You can address this issue in two ways:</p>

<ol>
<li><p>Use <code>CCACHE_CPP2</code>, as discussed in part 2.  With ccache 3.2, you
can now also put this into a configuration file: <code>run_second_cpp =
true</code> in <code>~/.ccache/ccache.conf</code></p></li>
<li><p>Turn off the warnings mentioned above: <code>-Wno-parentheses-equality</code>,
<code>-Wno-tautological-compare</code>, <code>-Wno-self-assign</code> (and any others you
might find).  One might think that these are actually useful warnings
that one might want to keep, but GCC doesn&rsquo;t warn about them, and if
you develop primarily with GCC, your code might contain these issues
anyway.  In particular, I have found that <code>-Wno-tautological-compare</code>
is necessary for legitimate code.</p></li>
</ol>


<p>I think <code>CCACHE_CPP2</code> is the way to go, for two reasons.  Firstly,
having to add more and more options to turn off warnings is obviously
somewhat stupid.  Secondly and more importantly, there is nothing
stopping GCC from adding warnings similar to Clang&rsquo;s that would
trigger on preprocessed versions of otherwise harmless C code.  Unless
they come up with a clever way to annotate the preprocessed code to
the effect of &ldquo;this code might look wrong to you, but it looked OK
before preprocessing, so don&rsquo;t warn about it&rdquo;, in a way that creates
<em>no</em> extra warnings and doesn&rsquo;t lose <em>any</em> warnings, I don&rsquo;t think
this issue can be solved.</p>

<h2>Speed!</h2>

<p>Now the question is, how much would globally setting <code>CCACHE_CPP2</code>
slow things down?</p>

<p>To test this, I have built PostgreSQL 9.4rc1 with <code>clang-3.5</code> and
<code>gcc-4.8</code> (not <code>gcc-4.9</code> because it creates some unrelated warnings
that I don&rsquo;t want to deal with here).  I have set <code>export
CCACHE_RECACHE=true</code> so that the cache is not read but new cache
entries are computed.  That way, the overhead of <code>ccache</code> on top of
the compiler is measured.</p>

<p>Results:</p>

<ul>
<li><code>clang-3.5</code>

<ul>
<li>Using <code>ccache</code> is 10% slower than not using it at all.</li>
<li>Using <code>ccache</code> with <code>CCACHE_CPP2</code> on is another 10% slower.</li>
</ul>
</li>
<li><code>gcc-4.8</code>

<ul>
<li>Using <code>ccache</code> is 19% slower than not using it at all.</li>
<li>Using <code>ccache</code> with <code>CCACHE_CPP2</code> is another 9% slower.</li>
</ul>
</li>
</ul>


<p>(There different percentages between <code>gcc</code> and <code>clang</code> arise because
<code>gcc</code> is faster than <code>clang</code> (yes, really, more on that in a future
post), but the overhead of <code>ccache</code> doesn&rsquo;t change.)</p>

<p>10% or so is not to be dismissed, but let&rsquo;s remember that this applies
only if there is a cache miss.  If everything is cached, both methods
do the same thing.  Also, if you use parallel make, the overhead is
divided by the number of parallel jobs.</p>

<p>With that in mind, I have decided to put the issue to rest for myself
and have made myself a <code>~/.ccache/ccache.conf</code> containing</p>

<pre><code>run_second_cpp = true
</code></pre>

<p>Now Clang or any other compiler should run without problems through
ccache.</p>

<h2>Color!</h2>

<p>There is one more piece of news in the new ccache release: Another
thing I talked about in part 1 was that <code>ccache</code> will disable the
colored output of <code>clang</code>, and I suggested workarounds.  This was
actually fixed in ccache 3.2, so the workarounds are no longer
necessary, and the above configuration change is really the only thing
to make Clang work smoothly with ccache.</p>
]]></content>
  </entry>
  
</feed>
