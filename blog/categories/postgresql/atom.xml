<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: postgresql | Peter Eisentraut]]></title>
  <link href="http://peter.eisentraut.org/blog/categories/postgresql/atom.xml" rel="self"/>
  <link href="http://peter.eisentraut.org/"/>
  <updated>2016-07-20T12:31:39-04:00</updated>
  <id>http://peter.eisentraut.org/</id>
  <author>
    <name><![CDATA[Peter Eisentraut]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using GNU GLOBAL with PostgreSQL]]></title>
    <link href="http://peter.eisentraut.org/blog/2016/07/20/using-gnu-global-with-postgresql"/>
    <updated>2016-07-20T08:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2016/07/20/using-gnu-global-with-postgresql</id>
    <content type="html"><![CDATA[<p>When you are coding in a source tree as big as PostgreSQL&rsquo;s, you will
at some point want to look into some kind of source code indexing.
It&rsquo;s often convenient not to bother, since <code>git grep</code> is actually
superfast.  But when you want to find where a function is defined
among all the call sites, some more intelligence is useful.</p>

<p>The traditional tools for this are <code>ctags</code> and <code>etags</code>, which create
index files intended for use by vi and Emacs, respectively.  The
PostgreSQL source tree has some customized support for these in the
tools <code>src/tools/make_ctags</code> and <code>src/tools/make_etags</code>.  Because
these tools operate on a directory level, those wrapper scripts create
a single tag file (named <code>tags</code> or <code>TAGS</code> respectively) in the
top-level directory and symlink it to all the other directories.  This
allows you to easily look for entries across the entire source tree.
But it&rsquo;s clearly a hack, and at least Emacs is often somewhat confused
by this setup.</p>

<p>But there is something much better that works very similarly:
<a href="https://www.gnu.org/software/global/">GNU GLOBAL</a>.  A main difference
is that GNU GLOBAL works on a project basis not on a directory basis,
so you don&rsquo;t need to do contortions to create and manage tags files
all over your source tree.  Also, GLOBAL can be used from the command
line, so you don&rsquo;t need to be an editor wizard to get started with it.
Plus, it appears to be much faster.</p>

<p>The whole thing is very simple.  Install the package, which is usually
called <code>global</code> and available in most operating system distributions.
To start, run</p>

<pre><code>$ gtags
</code></pre>

<p>in the top-level directory.  This creates the files <code>GPATH</code>, <code>GRTAGS</code>,
and <code>GTAGS</code>.</p>

<p>Then you can use <code>global</code> to search for stuff, like</p>

<pre><code>$ global elog
src/include/utils/elog.h
</code></pre>

<p>Or you can look for places a function is called:</p>

<pre><code>$ global -r write_stderr
</code></pre>

<p>You can run <code>global</code> in any directory.</p>

<p>Or how about you want to look at the code where something is defined:</p>

<pre><code>$ less -t elog
</code></pre>

<p>Note no file name is required.  (See the manual for the required setup
to make this work with <code>less</code>.)</p>

<p>Or of course use editor integration.  For Emacs, there is
<a href="https://github.com/leoliu/ggtags"><code>ggtags-mode</code></a>.</p>

<p>Here is some fine-tuning for use with the PostgreSQL source tree.
Generally, I don&rsquo;t want to index generated files.  For example, I
don&rsquo;t want to see hits in <code>gram.c</code>, only in <code>gram.y</code>.  Plus, you don&rsquo;t
want to index header files under <code>tmp_install</code>.  (Super annoying when
you use this to jump to a file to edit and later find that your edits
have been blown away by <code>make check</code>.)  But when you run <code>gtags</code> in a
partially built tree, it will index everything it finds.  To fix that,
I have restricted <code>gtags</code> to only index files that are registered in
Git, by first running</p>

<pre><code>git ls-files &gt;gtags.files
</code></pre>

<p>in the top-level directory.  Then <code>gtags</code> will only consider the
listed files.</p>

<p>This will also improve the workings of the Emacs mode, which will at
random times call <code>global -u</code> to update the tags.  If it finds a
<code>gtags.files</code> file, it will observe that and not index random files
lying around.</p>

<p>I have a shell alias <code>pgconfigure</code> which calls <code>configure</code> with a
bunch of options for easier typing.  It&rsquo;s basically something like</p>

<pre><code class="sh">pgconfigure() {
    ./configure --prefix=$(cd .. &amp;&amp; pwd)/pg-install --enable-debug --enable-cassert ...
}
</code></pre>

<p>At the end I call</p>

<pre><code class="sh">    git ls-files &gt;gtags.files
    gtags -i &amp;
</code></pre>

<p>to initialize the source tree for GNU GLOBAL, so it&rsquo;s always
there.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Check your pg_dump compression levels]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/12/07/check-your-pg-dump-compression-levels"/>
    <updated>2015-12-07T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/12/07/check-your-pg-dump-compression-levels</id>
    <content type="html"><![CDATA[<p>I was idly wondering what was taking <code>pg_dump</code> so long and noticed
that it always seemed to be pegged at 100% CPU usage on the client.
That was surprising because naively one might think that the
bottleneck are the server&rsquo;s or the client&rsquo;s disk or the network.
Profiling quickly revealed that the compression library zlib was
taking most of the run time on the client.  And indeed, turning
compression off caused <code>pg_dump</code> to fly without getting anywhere near
100% CPU.</p>

<p>When using the custom output format in <code>pg_dump</code> (<code>-Fc</code>), the output
is automatically compressed, using the same default level that <code>gzip</code>
uses.  By using the option <code>-Z</code>, one can select a compression level
between 0 (off) and 9 (highest).  Although it is not documented, the
default corresponds to level 6.</p>

<p>Some simple testing has shown that lowering the level from 6 to 1 can
speed up the dump run time by a factor of 3 or more while only
increasing the output size by 10%.  Even the levels in between give
significant speed increases with only minimal differences in output
size.  On the other hand, increasing the compression level to 9 only
decreases the output size by about 1% while causing slow downs by a
factor of 2.  (In this test, level 1 was about twice as slow as no
compression, but the output size was about 40% of the uncompressed
size.  So using at least some compression was still desirable.)</p>

<p>I encourage experimentation with these numbers.  I might actually
default my scripts to <code>-Z1</code> in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Have problems with PostgreSQL?  Try using Hive!]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/08/14/have-problems-with-postgresql-try-using-hive"/>
    <updated>2015-08-14T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/08/14/have-problems-with-postgresql-try-using-hive</id>
    <content type="html"><![CDATA[<p>So I had this PostgreSQL database that was getting a bit too big, and
since it was really only for analytics, I figured it would be a good
fit for putting in Hadoop+Hive instead.</p>

<p>(For those not completely familiar with this: Hadoop is sort of a job
tracker and distributed file system. Hive is an SQL-like layer on top
of that.  I know the cool kids are now using Spark.  Maybe for another
day.)</p>

<p>The first thing you need to learn about the Hadoop ecosystem is its
idiosyncratically fragmented structure.  With PostgreSQL, you
basically have the community website, the community mailing lists, the
community source code distribution, the community binaries, and a
handful of binaries made by Linux distributions.  If you search the
web for a problem with PostgreSQL, you will normally gets hits on one
or more of: the documentation, the mailing lists, third-party mirrors
of the mailing lists, or Stack Overflow.  With Hadoop, you have the
resources provided by the Apache Software Foundation, including the
source distribution, bug tracker, documentation, and then bunch of
commercial vendors with their parallel universes, including their own
mutually incompatible binary distributions, their own copy of the
documentation, their own mailing lists, their own bug trackers, etc.
When you search for a problem with Hadoop, you will typically get hits
from three separate copies of the documentation, about eight mailing
lists, fifteen tutorials, and one thousand blog posts.  And about 20
unanswered posts on Stack Overflow.  Different vendors also favor
different technology extensions.  So if, say, you read that you should
use some storage method, chances are it&rsquo;s not even supported in a
given distribution.</p>

<p>The next thing to know is that any information about Hadoop that is
older than about two years is obsolete.  Because they keep changing
everything from command names to basic architecture.  Don&rsquo;t even
bother reading old stuff.  Don&rsquo;t even bother reading anything.</p>

<p>So Hive.  The basic setup is actually fairly well documented.  You set
up a Hadoop cluster, HDFS, create a few directories.  Getting the
permissions sorted out during these initial steps is not easy, but it
seldom is.  So you can create a few tables, load some data, run a few
queries.</p>

<p>Nevermind that in its default configuration <code>hive</code> spits out about a
dozen warnings on every startup about deprecated parameters and jar
file conflicts.  This is apparently well known.  Look around in the
internet for hive examples.  They show the same output.  Apparently
the packaged versions of Hadoop and Hive are not tuned for each other.</p>

<p>Then you learn: In the default configuration, there can only be <em>one</em>
Hive session connected at once.  It doesn&rsquo;t tell you this.  Instead,
when the second session wants to connect, it tells you</p>

<pre><code>Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
</code></pre>

<p>followed by hundreds of lines of exception traces.  This is Hive-speak
for: &ldquo;there is already one session connected&rdquo;.</p>

<p>You see, Hive needs a, cough, cough, relational database to store its
schema.  By default, it uses embedded Derby, which allows only one
connection at a time.  If you want to connect more than one session at
once, you need to set up an external &ldquo;Hive metastore&rdquo; on a MySQL or
PostgreSQL database.</p>

<p>Nevermind that Derby can actually run in server mode.  That&rsquo;s
apparently not supported by Hive.</p>

<p>So I had a PostgreSQL database handy and tried to set that up.  I
installed the PostgreSQL JDBC driver, created an external database,
changed the Hive configuration to use an external database.</p>

<p>At this point, it turned out that the PostgreSQL JDBC driver was
broken, so I had to downgrade to an older version.  (The driver has
since been fixed.)</p>

<p>After I got one that was working, Hive kept complaining that it
couldn&rsquo;t find a driver that matches the JDBC URL
<code>jdbc:postgresql://somehost/hive_metastore</code>.  The PostgreSQL JDBC
driver explains in detail how to load the driver, but how do I get
that into Hive?</p>

<p>The first suggestion from the internet was to add something like this
to <code>.hiverc</code>:</p>

<pre><code>add jar /usr/share/java/postgresql-jdbc.jar;
</code></pre>

<p>That doesn&rsquo;t work.  Remember, don&rsquo;t believe anything you read on the
internet.</p>

<p>In between I even tried download the MySQL JDBC driver (no, I don&rsquo;t
want to sign in with my Oracle account), but it had the same problem.</p>

<p><code>hive</code> is actually a shell script which loads another shell script
which loads a bunch of other shell scripts, which eventually starts
<code>java</code>.  After randomly poking around I determined that if I did</p>

<pre><code>export HIVE_AUX_JARS_PATH=/usr/share/java/
</code></pre>

<p>it would pick up the jar files in that directory.  OK, that worked.</p>

<p>Now I can create tables, load data, run simple queries, from more than
one session.  So I could do</p>

<pre><code>SELECT * FROM mytable;
</code></pre>

<p>But as soon as I ran</p>

<pre><code>SELECT count(*) FROM mytable;
</code></pre>

<p>it crapped out again:</p>

<pre><code>java.io.FileNotFoundException: File does not exist: hdfs://namenode/usr/share/java/jline-0.9.94.jar
</code></pre>

<p>So it&rsquo;s apparently looking for some jar file on HDFS rather than the
regular file system.  Some totally unrelated jar file, too.</p>

<p>The difference between the two queries is that the first one is
answered by just dumping out data locally, whereas the second one
generates a distributed map-reduce job.  It doesn&rsquo;t tell you that
beforehand, of course.  Or even afterwards.</p>

<p>After a while I figured that this must have something to do with the
<code>HIVE_AUX_JARS_PATH</code> setting.  I changed that to</p>

<pre><code>export HIVE_AUX_JARS_PATH=/usr/share/java/postgresql-jdbc.jar;
</code></pre>

<p>so it would look at only one file, and sure enough it now complains</p>

<pre><code>java.io.FileNotFoundException: File does not exist: hdfs://namenode/usr/share/java/postgresql-jdbc.jar
</code></pre>

<p>Apparently, the <code>HIVE_AUX_JARS_PATH</code> facility is for adding jars that
contain user-defined functions that you need at run time.  As far as I
can tell, there is no separate setting for adding jars that you only
need locally.</p>

<p>There are workarounds for that on the internet, of varying
bizarreness, none of which worked.  Remember, don&rsquo;t believe anything
you read on the internet.</p>

<p>In the end, I indulged it and just uploaded that jar file into HDFS.
Whatever.</p>

<p>I then put my data loading job into cron, which quickly crapped out
because <code>JAVA_HOME</code> is not set in the cron environment.  After that
was fixed, I let my data loading jobs run for a while.</p>

<p>Later, I wanted clear out the previous experiments, drop all tables,
and start again.  Apparently, dropping a table in Hive takes a very
long time.  Actually, no.  When you use PostgreSQL for the Hive
metastore, any attempt to drop a table will <em>hang indefinitely</em>.</p>

<p><a href="https://www.mail-archive.com/user@hive.apache.org/msg00515.html">Someone</a>
summarized the issue:</p>

<blockquote><p>You are the first person I have heard of using postgres. I commend
you for not succumbing to the social pressure and just installing
mysql.  However I would advice succumbing to the social pressure and
using either derby or mysql.</p>

<p>The reason I say this is because jpox
&ldquo;has support&rdquo; for a number of data stores (M$ SQL server) however,
people have run into issues with them. Databases other then derby
and mysql &lsquo;should work&rsquo; but are generally untested.</p></blockquote>

<p>Not that actually testing it would take much work.  It&rsquo;s not like Hive
doesn&rsquo;t have any tests.  Just add some tests.</p>

<p>It&rsquo;s funny that they didn&rsquo;t write &ldquo;You are the first person I have
heard of using hive&rdquo;.  Clearly, nobody has ever actually used this.</p>

<p>Anyway, somehow I ended up creating the metastore schema manually by
copying and pasting various pieces from the internet and raw files.
Shudder.</p>

<p>How about more fun?  Here is a run-of-the-mill SQL parse error:</p>

<pre><code>NoViableAltException(26@[221:1: constant : ( Number | dateLiteral | StringLiteral | stringLiteralSequence | BigintLiteral | SmallintLiteral | TinyintLiteral | DecimalLiteral | charSetStringLiteral | booleanValue );])
    at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
    at org.antlr.runtime.DFA.predict(DFA.java:116)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.constant(HiveParser_IdentifiersParser.java:4377)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.partitionVal(HiveParser_IdentifiersParser.java:8444)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.partitionSpec(HiveParser_IdentifiersParser.java:8283)
    at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.tableOrPartition(HiveParser_IdentifiersParser.java:8161)
    at org.apache.hadoop.hive.ql.parse.HiveParser.tableOrPartition(HiveParser.java:31397)
    at org.apache.hadoop.hive.ql.parse.HiveParser.insertClause(HiveParser.java:30914)
    at org.apache.hadoop.hive.ql.parse.HiveParser.regular_body(HiveParser.java:29076)
    at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatement(HiveParser.java:28968)
    at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:28762)
    at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1238)
    at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:938)
    at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:190)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1000)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)
    at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)
    at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:622)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
FAILED: ParseException line 4:20 cannot recognize input near 'year' '(' 'event_timestamp' in constant
</code></pre>

<p>In PostgreSQL, this might say</p>

<pre><code>syntax error at or near "("
</code></pre>

<p>with a pointer to the actual query.</p>

<p>I just put a function call somewhere where it didn&rsquo;t belong.  The
documentation is very terse and confusing about a lot of these things.
And the documentation is kept as a series of wiki pages.</p>

<p>So now I have a really slow distributed version of my PostgreSQL
database, which stores its schema in another PostgreSQL database.
I forgot why I needed that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Storing URIs in PostgreSQL]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/04/16/storing-uris-in-postgresql"/>
    <updated>2015-04-16T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/04/16/storing-uris-in-postgresql</id>
    <content type="html"><![CDATA[<p>About two months ago, this happened:</p>

<blockquote class="twitter-tweet" lang="en"><p>What form of bribery would be required to convince someone to write an `email` and `url` data type for Postgres, by the way? Any takers?</p>&mdash; Peter van Hardenberg (@pvh) <a href="https://twitter.com/pvh/status/567395527357001728">February 16, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>And a few hours later:</p>

<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" lang="en"><p><a href="https://twitter.com/pvh">@pvh</a> <a href="https://t.co/NNxHRsHudz">https://t.co/NNxHRsHudz</a></p>&mdash; Peter Eisentraut (@petereisentraut) <a href="https://twitter.com/petereisentraut/status/567902299194073089">February 18, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>It took a few more hours and days after this to refine some details,
but I have now tagged the first
<a href="https://github.com/petere/pguri/releases/tag/1.20150415">release</a> of
this extension.  Give it a try and let me know what you think.  Bug
reports and feature requests are welcome.</p>

<p>(I chose to name the data type <code>uri</code> instead of <code>url</code>, as originally
suggested, because that is more correct and matches what the parsing
library calls it.  One could create a domain if one prefers the other
name or if one wants to restrict the values to certain kinds of URIs
or URLs.)</p>

<p>(If you are interested in storing email addresses,
<a href="https://wiki.postgresql.org/wiki/Email_address_parsing">here</a> is an
idea.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Retrieving PgBouncer statistics via dblink]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/03/25/retrieving-pgbouncer-statistics-via-dblink"/>
    <updated>2015-03-25T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/03/25/retrieving-pgbouncer-statistics-via-dblink</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.postgresql.org/wiki/PgBouncer">PgBouncer</a> has a virtual
database called <code>pgbouncer</code>.  If you connect to that you can run
special SQL-like commands, for example</p>

<pre><code>$ psql -p 6432 pgbouncer
=# SHOW pools;
┌─[ RECORD 1 ]───────────┐
│ database   │ pgbouncer │
│ user       │ pgbouncer │
│ cl_active  │ 1         │
│ cl_waiting │ 0         │
│ sv_active  │ 0         │
│ sv_idle    │ 0         │
│ sv_used    │ 0         │
│ sv_tested  │ 0         │
│ sv_login   │ 0         │
│ maxwait    │ 0         │
└────────────┴───────────┘
</code></pre>

<p>This is quite nice, but unfortunately, you cannot run full SQL queries
against that data.  So you couldn&rsquo;t do something like</p>

<pre><code class="sql">SELECT * FROM pgbouncer.pools WHERE maxwait &gt; 0;
</code></pre>

<p>Well, here is a way: From a regular PostgreSQL database, connect to
PgBouncer using dblink.  For each <code>SHOW</code> command provided by
PgBouncer, create a view.  Then that SQL query actually works.</p>

<p>But before you start doing that, I have already done that here:</p>

<p><div><script src='https://gist.github.com/b4e2aa7cb4a073e07630.js?file=pgbouncer-schema.sql'></script>
<noscript><pre><code>CREATE EXTENSION dblink;


-- customize start
CREATE SERVER pgbouncer FOREIGN DATA WRAPPER dblink_fdw OPTIONS (host &#39;localhost&#39;,
                                                                 port &#39;6432&#39;,
                                                                 dbname &#39;pgbouncer&#39;);

CREATE USER MAPPING FOR PUBLIC SERVER pgbouncer OPTIONS (user &#39;pgbouncer&#39;);
-- customize stop


CREATE SCHEMA pgbouncer;

CREATE VIEW pgbouncer.clients AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show clients&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text);

CREATE VIEW pgbouncer.config AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show config&#39;) AS _(key text, value text, changeable boolean);

CREATE VIEW pgbouncer.databases AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show databases&#39;) AS _(name text, host text, port int, database text, force_user text, pool_size int, reserve_pool int);

CREATE VIEW pgbouncer.lists AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show lists&#39;) AS _(list text, items int);

CREATE VIEW pgbouncer.pools AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show pools&#39;) AS _(database text, &quot;user&quot; text, cl_active int, cl_waiting int, sv_active int, sv_idle int, sv_used int, sv_tested int, sv_login int, maxwait int);

CREATE VIEW pgbouncer.servers AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show servers&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text);

CREATE VIEW pgbouncer.sockets AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show sockets&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text,
                                                           recv_pos int, pkt_pos int, pkt_remain int, send_pos int, send_remain int, pkt_avail int, send_avail int);
</code></pre></noscript></div>
</p>

<p>Here is another useful example.  If you&rsquo;re tracing back connections
from the database server through PgBouncer to the client, try this:</p>

<pre><code class="sql">SELECT * FROM pgbouncer.servers LEFT JOIN pgbouncer.clients ON servers.link = clients.ptr;
</code></pre>

<p>Unfortunately, different versions of PgBouncer return a different
number of columns for some commands.  Then you will need different
view definitions.  I haven&rsquo;t determined a way to handle that
elegantly.</p>
]]></content>
  </entry>
  
</feed>
