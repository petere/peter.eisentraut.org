<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Peter Eisentraut]]></title>
  <link href="http://peter.eisentraut.org/atom.xml" rel="self"/>
  <link href="http://peter.eisentraut.org/"/>
  <updated>2015-12-07T20:59:12-05:00</updated>
  <id>http://peter.eisentraut.org/</id>
  <author>
    <name><![CDATA[Peter Eisentraut]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Check your pg_dump compression levels]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/12/07/check-your-pg-dump-compression-levels"/>
    <updated>2015-12-07T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/12/07/check-your-pg-dump-compression-levels</id>
    <content type="html"><![CDATA[<p>I was idly wondering what was taking <code>pg_dump</code> so long and noticed
that it always seemed to be pegged at 100% CPU usage on the client.
That was surprising because naively one might think that the
bottleneck are the server&rsquo;s or the client&rsquo;s disk or the network.
Profiling quickly revealed that the compression library zlib was
taking most of the run time on the client.  And indeed, turning
compression off caused <code>pg_dump</code> to fly without getting anywhere near
100% CPU.</p>

<p>When using the custom output format in <code>pg_dump</code> (<code>-Fc</code>), the output
is automatically compressed, using the same default level that <code>gzip</code>
uses.  By using the option <code>-Z</code>, one can select a compression level
between 0 (off) and 9 (highest).  Although it is not documented, the
default corresponds to level 6.</p>

<p>Some simple testing has shown that lowering the level from 6 to 1 can
speed up the dump run time by a factor of 3 or more while only
increasing the output size by 10%.  Even the levels in between give
significant speed increases with only minimal differences in output
size.  On the other hand, increasing the compression level to 9 only
decreases the output size by about 1% while causing slow downs by a
factor of 2.  (In this test, level 1 was about twice as slow as no
compression, but the output size was about 40% of the uncompressed
size.  So using at least some compression was still desirable.)</p>

<p>I encourage experimentation with these numbers.  I might actually
default my scripts to <code>-Z1</code> in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Have problems with PostgreSQL?  Try using Hive!]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/08/14/have-problems-with-postgresql-try-using-hive"/>
    <updated>2015-08-14T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/08/14/have-problems-with-postgresql-try-using-hive</id>
    <content type="html"><![CDATA[<p>So I had this PostgreSQL database that was getting a bit too big, and
since it was really only for analytics, I figured it would be a good
fit for putting in Hadoop+Hive instead.</p>

<p>(For those not completely familiar with this: Hadoop is sort of a job
tracker and distributed file system. Hive is an SQL-like layer on top
of that.  I know the cool kids are now using Spark.  Maybe for another
day.)</p>

<p>The first thing you need to learn about the Hadoop ecosystem is its
idiosyncratically fragmented structure.  With PostgreSQL, you
basically have the community website, the community mailing lists, the
community source code distribution, the community binaries, and a
handful of binaries made by Linux distributions.  If you search the
web for a problem with PostgreSQL, you will normally gets hits on one
or more of: the documentation, the mailing lists, third-party mirrors
of the mailing lists, or Stack Overflow.  With Hadoop, you have the
resources provided by the Apache Software Foundation, including the
source distribution, bug tracker, documentation, and then bunch of
commercial vendors with their parallel universes, including their own
mutually incompatible binary distributions, their own copy of the
documentation, their own mailing lists, their own bug trackers, etc.
When you search for a problem with Hadoop, you will typically get hits
from three separate copies of the documentation, about eight mailing
lists, fifteen tutorials, and one thousand blog posts.  And about 20
unanswered posts on Stack Overflow.  Different vendors also favor
different technology extensions.  So if, say, you read that you should
use some storage method, chances are it&rsquo;s not even supported in a
given distribution.</p>

<p>The next thing to know is that any information about Hadoop that is
older than about two years is obsolete.  Because they keep changing
everything from command names to basic architecture.  Don&rsquo;t even
bother reading old stuff.  Don&rsquo;t even bother reading anything.</p>

<p>So Hive.  The basic setup is actually fairly well documented.  You set
up a Hadoop cluster, HDFS, create a few directories.  Getting the
permissions sorted out during these initial steps is not easy, but it
seldom is.  So you can create a few tables, load some data, run a few
queries.</p>

<p>Nevermind that in its default configuration <code>hive</code> spits out about a
dozen warnings on every startup about deprecated parameters and jar
file conflicts.  This is apparently well known.  Look around in the
internet for hive examples.  They show the same output.  Apparently
the packaged versions of Hadoop and Hive are not tuned for each other.</p>

<p>Then you learn: In the default configuration, there can only be <em>one</em>
Hive session connected at once.  It doesn&rsquo;t tell you this.  Instead,
when the second session wants to connect, it tells you</p>

<pre><code>Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
</code></pre>

<p>followed by hundreds of lines of exception traces.  This is Hive-speak
for: &ldquo;there is already one session connected&rdquo;.</p>

<p>You see, Hive needs a, cough, cough, relational database to store its
schema.  By default, it uses embedded Derby, which allows only one
connection at a time.  If you want to connect more than one session at
once, you need to set up an external &ldquo;Hive metastore&rdquo; on a MySQL or
PostgreSQL database.</p>

<p>Nevermind that Derby can actually run in server mode.  That&rsquo;s
apparently not supported by Hive.</p>

<p>So I had a PostgreSQL database handy and tried to set that up.  I
installed the PostgreSQL JDBC driver, created an external database,
changed the Hive configuration to use an external database.</p>

<p>At this point, it turned out that the PostgreSQL JDBC driver was
broken, so I had to downgrade to an older version.  (The driver has
since been fixed.)</p>

<p>After I got one that was working, Hive kept complaining that it
couldn&rsquo;t find a driver that matches the JDBC URL
<code>jdbc:postgresql://somehost/hive_metastore</code>.  The PostgreSQL JDBC
driver explains in detail how to load the driver, but how do I get
that into Hive?</p>

<p>The first suggestion from the internet was to add something like this
to <code>.hiverc</code>:</p>

<pre><code>add jar /usr/share/java/postgresql-jdbc.jar;
</code></pre>

<p>That doesn&rsquo;t work.  Remember, don&rsquo;t believe anything you read on the
internet.</p>

<p>In between I even tried download the MySQL JDBC driver (no, I don&rsquo;t
want to sign in with my Oracle account), but it had the same problem.</p>

<p><code>hive</code> is actually a shell script which loads another shell script
which loads a bunch of other shell scripts, which eventually starts
<code>java</code>.  After randomly poking around I determined that if I did</p>

<pre><code>export HIVE_AUX_JARS_PATH=/usr/share/java/
</code></pre>

<p>it would pick up the jar files in that directory.  OK, that worked.</p>

<p>Now I can create tables, load data, run simple queries, from more than
one session.  So I could do</p>

<pre><code>SELECT * FROM mytable;
</code></pre>

<p>But as soon as I ran</p>

<pre><code>SELECT count(*) FROM mytable;
</code></pre>

<p>it crapped out again:</p>

<pre><code>java.io.FileNotFoundException: File does not exist: hdfs://namenode/usr/share/java/jline-0.9.94.jar
</code></pre>

<p>So it&rsquo;s apparently looking for some jar file on HDFS rather than the
regular file system.  Some totally unrelated jar file, too.</p>

<p>The difference between the two queries is that the first one is
answered by just dumping out data locally, whereas the second one
generates a distributed map-reduce job.  It doesn&rsquo;t tell you that
beforehand, of course.  Or even afterwards.</p>

<p>After a while I figured that this must have something to do with the
<code>HIVE_AUX_JARS_PATH</code> setting.  I changed that to</p>

<pre><code>export HIVE_AUX_JARS_PATH=/usr/share/java/postgresql-jdbc.jar;
</code></pre>

<p>so it would look at only one file, and sure enough it now complains</p>

<pre><code>java.io.FileNotFoundException: File does not exist: hdfs://namenode/usr/share/java/postgresql-jdbc.jar
</code></pre>

<p>Apparently, the <code>HIVE_AUX_JARS_PATH</code> facility is for adding jars that
contain user-defined functions that you need at run time.  As far as I
can tell, there is no separate setting for adding jars that you only
need locally.</p>

<p>There are workarounds for that on the internet, of varying
bizarreness, none of which worked.  Remember, don&rsquo;t believe anything
you read on the internet.</p>

<p>In the end, I indulged it and just uploaded that jar file into HDFS.
Whatever.</p>

<p>I then put my data loading job into cron, which quickly crapped out
because <code>JAVA_HOME</code> is not set in the cron environment.  After that
was fixed, I let my data loading jobs run for a while.</p>

<p>Later, I wanted clear out the previous experiments, drop all tables,
and start again.  Apparently, dropping a table in Hive takes a very
long time.  Actually, no.  When you use PostgreSQL for the Hive
metastore, any attempt to drop a table will <em>hang indefinitely</em>.</p>

<p><a href="https://www.mail-archive.com/user@hive.apache.org/msg00515.html">Someone</a>
summarized the issue:</p>

<blockquote><p>You are the first person I have heard of using postgres. I commend
you for not succumbing to the social pressure and just installing
mysql.  However I would advice succumbing to the social pressure and
using either derby or mysql.</p>

<p>The reason I say this is because jpox
&ldquo;has support&rdquo; for a number of data stores (M$ SQL server) however,
people have run into issues with them. Databases other then derby
and mysql &lsquo;should work&rsquo; but are generally untested.</p></blockquote>

<p>Not that actually testing it would take much work.  It&rsquo;s not like Hive
doesn&rsquo;t have any tests.  Just add some tests.</p>

<p>It&rsquo;s funny that they didn&rsquo;t write &ldquo;You are the first person I have
heard of using hive&rdquo;.  Clearly, nobody has ever actually used this.</p>

<p>Anyway, somehow I ended up creating the metastore schema manually by
copying and pasting various pieces from the internet and raw files.
Shudder.</p>

<p>How about more fun?  Here is a run-of-the-mill SQL parse error:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>NoViableAltException(26@[221:1: constant : ( Number | dateLiteral | StringLiteral | stringLiteralSequence | BigintLiteral | SmallintLiteral | TinyintLiteral | DecimalLiteral | charSetStringLiteral | booleanValue );])
</span><span class='line'>  at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
</span><span class='line'>  at org.antlr.runtime.DFA.predict(DFA.java:116)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.constant(HiveParser_IdentifiersParser.java:4377)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.partitionVal(HiveParser_IdentifiersParser.java:8444)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.partitionSpec(HiveParser_IdentifiersParser.java:8283)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.tableOrPartition(HiveParser_IdentifiersParser.java:8161)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.tableOrPartition(HiveParser.java:31397)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.insertClause(HiveParser.java:30914)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.regular_body(HiveParser.java:29076)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatement(HiveParser.java:28968)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:28762)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1238)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:938)
</span><span class='line'>  at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:190)
</span><span class='line'>  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)
</span><span class='line'>  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
</span><span class='line'>  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1000)
</span><span class='line'>  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
</span><span class='line'>  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
</span><span class='line'>  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</span><span class='line'>  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
</span><span class='line'>  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</span><span class='line'>  at java.lang.reflect.Method.invoke(Method.java:622)
</span><span class='line'>  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
</span><span class='line'>FAILED: ParseException line 4:20 cannot recognize input near 'year' '(' 'event_timestamp' in constant</span></code></pre></td></tr></table></div></figure>


<p>In PostgreSQL, this might say</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>syntax error at or near "("</span></code></pre></td></tr></table></div></figure>


<p>with a pointer to the actual query.</p>

<p>I just put a function call somewhere where it didn&rsquo;t belong.  The
documentation is very terse and confusing about a lot of these things.
And the documentation is kept as a series of wiki pages.</p>

<p>So now I have a really slow distributed version of my PostgreSQL
database, which stores its schema in another PostgreSQL database.
I forgot why I needed that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Storing URIs in PostgreSQL]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/04/16/storing-uris-in-postgresql"/>
    <updated>2015-04-16T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/04/16/storing-uris-in-postgresql</id>
    <content type="html"><![CDATA[<p>About two months ago, this happened:</p>

<blockquote class="twitter-tweet" lang="en"><p>What form of bribery would be required to convince someone to write an `email` and `url` data type for Postgres, by the way? Any takers?</p>&mdash; Peter van Hardenberg (@pvh) <a href="https://twitter.com/pvh/status/567395527357001728">February 16, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>And a few hours later:</p>

<blockquote class="twitter-tweet" data-conversation="none" data-cards="hidden" lang="en"><p><a href="https://twitter.com/pvh">@pvh</a> <a href="https://t.co/NNxHRsHudz">https://t.co/NNxHRsHudz</a></p>&mdash; Peter Eisentraut (@petereisentraut) <a href="https://twitter.com/petereisentraut/status/567902299194073089">February 18, 2015</a></blockquote>


<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>It took a few more hours and days after this to refine some details,
but I have now tagged the first
<a href="https://github.com/petere/pguri/releases/tag/1.20150415">release</a> of
this extension.  Give it a try and let me know what you think.  Bug
reports and feature requests are welcome.</p>

<p>(I chose to name the data type <code>uri</code> instead of <code>url</code>, as originally
suggested, because that is more correct and matches what the parsing
library calls it.  One could create a domain if one prefers the other
name or if one wants to restrict the values to certain kinds of URIs
or URLs.)</p>

<p>(If you are interested in storing email addresses,
<a href="https://wiki.postgresql.org/wiki/Email_address_parsing">here</a> is an
idea.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Retrieving PgBouncer statistics via dblink]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/03/25/retrieving-pgbouncer-statistics-via-dblink"/>
    <updated>2015-03-25T20:00:00-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/03/25/retrieving-pgbouncer-statistics-via-dblink</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.postgresql.org/wiki/PgBouncer">PgBouncer</a> has a virtual
database called <code>pgbouncer</code>.  If you connect to that you can run
special SQL-like commands, for example</p>

<pre><code>$ psql -p 6432 pgbouncer
=# SHOW pools;
┌─[ RECORD 1 ]───────────┐
│ database   │ pgbouncer │
│ user       │ pgbouncer │
│ cl_active  │ 1         │
│ cl_waiting │ 0         │
│ sv_active  │ 0         │
│ sv_idle    │ 0         │
│ sv_used    │ 0         │
│ sv_tested  │ 0         │
│ sv_login   │ 0         │
│ maxwait    │ 0         │
└────────────┴───────────┘
</code></pre>

<p>This is quite nice, but unfortunately, you cannot run full SQL queries
against that data.  So you couldn&rsquo;t do something like</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">pgbouncer</span><span class="p">.</span><span class="n">pools</span> <span class="k">WHERE</span> <span class="n">maxwait</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Well, here is a way: From a regular PostgreSQL database, connect to
PgBouncer using dblink.  For each <code>SHOW</code> command provided by
PgBouncer, create a view.  Then that SQL query actually works.</p>

<p>But before you start doing that, I have already done that here:</p>

<div><script src='https://gist.github.com/b4e2aa7cb4a073e07630.js?file=pgbouncer-schema.sql'></script>
<noscript><pre><code>CREATE EXTENSION dblink;


-- customize start
CREATE SERVER pgbouncer FOREIGN DATA WRAPPER dblink_fdw OPTIONS (host &#39;localhost&#39;,
                                                                 port &#39;6432&#39;,
                                                                 dbname &#39;pgbouncer&#39;);

CREATE USER MAPPING FOR PUBLIC SERVER pgbouncer OPTIONS (user &#39;pgbouncer&#39;);
-- customize stop


CREATE SCHEMA pgbouncer;

CREATE VIEW pgbouncer.clients AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show clients&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text);

CREATE VIEW pgbouncer.config AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show config&#39;) AS _(key text, value text, changeable boolean);

CREATE VIEW pgbouncer.databases AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show databases&#39;) AS _(name text, host text, port int, database text, force_user text, pool_size int, reserve_pool int);

CREATE VIEW pgbouncer.lists AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show lists&#39;) AS _(list text, items int);

CREATE VIEW pgbouncer.pools AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show pools&#39;) AS _(database text, &quot;user&quot; text, cl_active int, cl_waiting int, sv_active int, sv_idle int, sv_used int, sv_tested int, sv_login int, maxwait int);

CREATE VIEW pgbouncer.servers AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show servers&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text);

CREATE VIEW pgbouncer.sockets AS
    SELECT * FROM dblink(&#39;pgbouncer&#39;, &#39;show sockets&#39;) AS _(type text, &quot;user&quot; text, database text, state text, addr text, port int, local_addr text, local_port int, connect_time timestamp with time zone, request_time timestamp with time zone, ptr text, link text,
                                                           recv_pos int, pkt_pos int, pkt_remain int, send_pos int, send_remain int, pkt_avail int, send_avail int);
</code></pre></noscript></div>


<p>Here is another useful example.  If you&rsquo;re tracing back connections
from the database server through PgBouncer to the client, try this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">pgbouncer</span><span class="p">.</span><span class="n">servers</span> <span class="k">LEFT</span> <span class="k">JOIN</span> <span class="n">pgbouncer</span><span class="p">.</span><span class="n">clients</span> <span class="k">ON</span> <span class="n">servers</span><span class="p">.</span><span class="n">link</span> <span class="o">=</span> <span class="n">clients</span><span class="p">.</span><span class="n">ptr</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Unfortunately, different versions of PgBouncer return a different
number of columns for some commands.  Then you will need different
view definitions.  I haven&rsquo;t determined a way to handle that
elegantly.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The history of replication in PostgreSQL]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/03/03/the-history-of-replication-in-postgresql"/>
    <updated>2015-03-03T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/03/03/the-history-of-replication-in-postgresql</id>
    <content type="html"><![CDATA[<h2>2001: PostgreSQL 7.1: write-ahead log</h2>

<p>PostgreSQL 7.1 introduced the write-ahead log (WAL).  Before that
release, all open data files had to be fsynced on every commit, which
is very slow.  Slow fsyncing is still a problem today, but now we&rsquo;re
only worried about fsyncing the WAL, and fsyncing the data files
during the checkpoint process.  Back then, we had to fsync
<em>everything</em> <em>all the time</em>.</p>

<p>In the original design of university POSTGRES, the lack of a log was
intentional, and contrasted with heavily log-based architectures such
as Oracle.  In Oracle, you need the log to roll back changes.  In
PostgreSQL, the nonoverwriting storage system takes care of that.  But
probably nobody thought about implications for fsyncing back then.</p>

<p>Note that the WAL was really just an implementation detail at this
point.  You couldn&rsquo;t read or archive it.</p>

<h2>2004: Slony</h2>

<p>Just for context: Slony-I 1.0 was
<a href="http://lists.slony.info/pipermail/slony1-general/2004-July/000106.html">released</a>
in July 2004.</p>

<h2>2005: PostgreSQL 8.0: point-in-time recovery</h2>

<p>PostgreSQL 8.0 added the possibility to copy the WAL somewhere else,
and later play it back, either all the way or to a particular point in
time, hence the name point-in-time recovery (PITR) for this feature.
This feature was mainly intended to relieve <code>pg_dump</code> as a backup
method.  Until then, the only backup method was a full dump, which
would get impractical as databases grew.  Hence this method to take an
occasional base backup, which is the expensive part, and then add on
parts of the WAL, which is cheaper.</p>

<p>The basic configuration mechanisms that we still use today, for
example the <code>recovery.conf</code> file, were introduced as part of this
feature.</p>

<p>But still no replication here.</p>

<h2>2008: PostgreSQL 8.3: pg_standby</h2>

<p>Crafty people eventually figured that if you archived WAL on one
server and at the same time &ldquo;recovered&rdquo; endlessly on another, you&rsquo;d
have a replication setup.  You could probably have set this up with
your own scripts as early as 8.0, but PostgreSQL 8.3 added the
<code>pg_standby</code> program into <code>contrib</code>, which gave everyone a standard
tool.  So, arguably, 8.3 is the first release that contained a
semblance of a built-in replication solution.</p>

<p>The standby server was in permanent recovery until promotion, so it
couldn&rsquo;t be read from as it was replicating.  This is what we&rsquo;d now
call a warm standby.</p>

<p>I think a lot of PostgreSQL 8.3 installations refuse to die, because
this is the first version where you could easily have a reasonably
up-to-date reserve server without resorting to complicated and
sometimes problematic tools like Slony or DRBD.</p>

<h2>2010: PostgreSQL 9.0: hot standby, streaming replication</h2>

<p>In PostgreSQL 9.0, two important replication features arrived
completely independently.  First, the possibility to connect to a
standby server in read-only mode, making it a so-called hot standby.
Whereas before, a standby server was really mainly useful only as a
reserve in case the primary server failed, with hot standby you could
use secondary servers to spread out read-only loads. Second, instead
of relying solely on the WAL archive and recovery functionalities to
transport WAL data, a standby server could connect directly to the
primary server via the existing libpq protocol and obtain WAL data
that way, so-called streaming replication.  The primary use in this
release was that the standby could be more up to date, possibly within
seconds, rather than several minutes with the archive-based approach.
For a robust setup, you would still need to set up an archive.  But
streaming replication was also a forward-looking feature that would
eventually make replication setups easier, by reducing the reliance on
the old archiving mechanisms.</p>

<p>PostgreSQL 9.0 was the first release where one could claim that
PostgreSQL &ldquo;supports replication&rdquo; without having to make
qualifications or excuses.  Although it is scheduled to go EOL later
this year, I expect this release will continue to live for a long
time.</p>

<h2>2011: PostgreSQL 9.1: pg_basebackup, synchronous replication</h2>

<p><code>pg_basebackup</code> was one of the features facilitated by streaming
replication that made things easier.  Instead of having to use
external tools like <code>rsync</code> for base backups, <code>pg_basebackup</code> would
use a normal libpq connection to pull down a base backup, thus
avoiding complicated connection and authentication setups for external
tools.  (Some people continue to favor <code>rsync</code> because it is faster
for them.)</p>

<p>PostgreSQL 9.1 also added synchronous replication, which ensures that
data is replicated to the designated synchronous standby before a
<code>COMMIT</code> reports success.  This feature is frequently misunderstood by
users.  While it ensures that your data is on at least two servers at
all times, it might actually reduce the availability of your system,
because if the standby server goes down, the primary will also go
down, unless you have a third server available to take over the
synchronous standby duty.</p>

<p>Less widely know perhaps is that PostgreSQL 9.1 also added the
<code>pg_last_xact_replay_timestamp</code> function for easy monitoring of
standby lag.</p>

<p>In my experience, the availability of <code>pg_basebackup</code> and
<code>pg_last_xact_replay_timestamp</code> make PostgreSQL 9.1 the first release
were managing replication was reasonably easy.  Go back further, and
you might feel constrained by the available tools.  But in 9.1, it&rsquo;s
not that much different from what is available in the most recent
releases.</p>

<h2>2012: PostgreSQL 9.2: cascading replication</h2>

<p>Not as widely acclaimed, more for the Slony buffs perhaps,
PostgreSQL 9.2 allowed standbys to fetch their streaming replication
data from other standbys.  A particular consequence of that is that
<code>pg_basebackup</code> could copy from a standby server, thus taking the load
off the primary server for setting up a new standby or standalone
copy.</p>

<h2>2013: PostgreSQL 9.3: standby can follow timeline switch</h2>

<p>This did not even make it into the release note highlights.  In
PostgreSQL 9.3, when a primary has two standbys, and one of the
standbys is promoted, the other standby can just keep following the
new primary.  In previous releases, the second standby would have to
be rebuilt.  This improvement makes dynamic infrastructure changes
much simpler.  Not only does it eliminate the time, annoyance, and
performance impact of setting up a new standby, more importantly it
avoids the situation that after a promotion, you don&rsquo;t have any up to
update standbys at all for a while.</p>

<h2>2014: PostgreSQL 9.4: replication slots, logical decoding</h2>

<p>Logical decoding got all the press for PostgreSQL 9.4, but I think
replication slots are the major feature, possibly the biggest
replication feature since PostgreSQL 9.0.  Note that while streaming
replication has gotten more sophisticated over the years, you still
needed a WAL archive for complete robustness.  That is because the
primary server didn&rsquo;t actually keep a list of its supposed standby
servers, it just streamed whatever WAL happened to be requested if it
happened to have it.  If the standby server fell behind sufficiently
far, streaming replication would fail, and recovery from the archive
would kick in.  If you didn&rsquo;t have an archive, the standby would then
no longer be able to catch up and would have to be rebuilt.  And this
archiving mechanism has essentially been unchanged since version 8.0,
when it was designed for an entirely different purpose.  So a
replication setup is actually quite messy: You have to configure an
access path from the primary to the standby (for archiving) and an
access path from the standby to the primary (for streaming).  And if
you wanted to do multiple standbys or cascading, maintaining the
archive could get really complicated.  Moreover, I think a lot of
archiving setups have problematic <code>archive_command</code> settings.  For
example, does your <code>archive_command</code> fsync the file on the receiving
side?  Probably not.</p>

<p>No more: In PostgreSQL 9.4, you can set up so-called replication
slots, which effectively means that you register a standby with the
primary, and the primary keeps around the WAL for each standby until
the standby has fetched it.  With this, you can completely get rid of
the archiving, unless you need it as a backup.</p>

<h2>2015? PostgreSQL 9.5? pg_rewind?</h2>

<p>One of the remaining problems is that promoting a standby leaves the
old primary unable to change course and follow the new primary.  If
you fail over because the old primary died, then that&rsquo;s not an issue.
But if you just want to swap primary and standby, perhaps because the
standby has more powerful hardware, then the old primary, now standby,
needs to be rebuilt completely from scratch.  Transforming an old
primary into a new standby without a completely new base backup is a
rather intricate problem, but a tool that can do it (currently named
<code>pg_rewind</code>) is proposed for inclusion into the next PostgreSQL
release.</p>

<h2>Beyond</h2>

<p>One of the problems that this evolution of replication has created is
that the configuration is rather idiosyncratic, quite complicated to
get right, and almost impossible to generalize sufficiently for
documentation, tutorials, and so on.  Dropping archiving with 9.4
might address some of these points, but configuring even just
streaming replication is still weird, even weirder if you don&rsquo;t know
how it got here.  You need to change several obscure configuration
parameters, some on the primary, some on the standby, some of which
require a hard restart of the primary server.  And then you need to
create a new configuration file <code>recovery.conf</code>, even though you don&rsquo;t
want to recover anything.  Making changes in this area is mostly a
complex political process, because the existing system has served
people well over many years, and coming up with a new system that is
obviously better and addresses all existing use cases is cumbersome.</p>

<p>Another issue is that all of this functionality has been bolted on to
the write-ahead log mechanism, and that constrains all the uses of the
write-ahead log in various ways.  For example, there are optimizations
that skip WAL logging in certain circumstances, but if you want
replication, you can&rsquo;t use them.  Who doesn&rsquo;t want replication?  Also,
the write-ahead log covers an entire database system and is all or
nothing.  You can&rsquo;t replicate only certain tables, for example, or
consolidate logs from two different sources.</p>

<p>How about not bolting all of this on to the WAL?  Have two different
logs for two different purposes.  This was discussed, especially
around the time streaming replication was built.  But then you&rsquo;d need
two logs that are <em>almost</em> the same.  And the WAL is by design a
bottleneck, so creating another log would probably create performance
problems.</p>

<p>Logical decoding breaks many of these restrictions and will likely be
the foundation for the next round of major replication features.
Examples include partial replication and multimaster replication, some
of which are being worked on right now.</p>

<p>What can we expect from plain WAL logging in the meantime?  Easier
configuration is certainly a common request.  But can we expect major
leaps on functionality?  Who knows.  At one point, something like hot
standby was thought to be nearly impossible.  So there might be
surprises still.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Listing screen sessions on login]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/02/16/listing-screen-sessions-on-login"/>
    <updated>2015-02-16T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/02/16/listing-screen-sessions-on-login</id>
    <content type="html"><![CDATA[<p>There is a lot of helpful information about <code>screen</code> out there, but I
haven&rsquo;t found anything about this.  I don&rsquo;t want to &ldquo;forget&rdquo; any
screen sessions, so I&rsquo;d like to be notified when I log into a box and
there are screens running for me.  Obviously, there is <code>screen -ls</code>,
but it needs to be wrapped in a bit logic so that it doesn&rsquo;t annoy
when there is no <code>screen</code> running or even installed.</p>

<p>After perusing the <code>screen</code> man page a little, I came up with this for
<code>.bash_profile</code> or <code>.zprofile</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="k">if</span> which screen &gt;/dev/null<span class="p">;</span> <span class="k">then</span>
</span><span class='line'>    screen -q -ls
</span><span class='line'>    <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -ge <span class="m">10</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span><span class='line'>        screen -ls
</span><span class='line'>    <span class="k">fi</span>
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure>


<p>The trick is that <code>-q</code> in conjuction with <code>-ls</code> gives you exit codes
about the current status of <code>screen</code>.</p>

<p>Here is an example of how this looks in practice:</p>

<pre><code>~$ ssh host
Last login: Fri Feb 13 11:30:10 2015 from 192.0.2.15
There is a screen on:
        31572.pts-0.foobar      (2015-02-15 13.03.21)   (Detached)
1 Socket in /var/run/screen/S-peter.

peter@host:~$ 
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Directing output to multiple files with zsh]]></title>
    <link href="http://peter.eisentraut.org/blog/2015/01/08/directing-output-to-multiple-files-with-zsh"/>
    <updated>2015-01-08T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2015/01/08/directing-output-to-multiple-files-with-zsh</id>
    <content type="html"><![CDATA[<p>Normally, this doesn&rsquo;t work as one might naively expect:</p>

<pre><code>program &gt; firstfile &gt; secondfile
</code></pre>

<p>The second redirection will override the first one.  You&rsquo;d have to use
an external tool to make this work, maybe something like:</p>

<pre><code>program | tee firstfile secondfile
</code></pre>

<p>But with zsh, this type of thing actually works.  It will duplicate
the output and write it to multiple files.</p>

<p>This feature also works with a combination of redirections and
pipes.  For example</p>

<pre><code>ls &gt; foo | grep bar
</code></pre>

<p>will write the complete directory listing into file <code>foo</code> <em>and</em> print
out files matching <code>bar</code> to the terminal.</p>

<p>That&rsquo;s great, but this feature pops up in unexpected places.</p>

<p>I have a shell function that checks whether a given command produces
any output on stderr:</p>

<pre><code>! myprog "$arg" 2&gt;&amp;1 &gt;/dev/null | grep .
</code></pre>

<p>The effect of this is:</p>

<ul>
<li>If no stderr is produced, the exit code is 0.</li>
<li>If stderr is produced, the exit code is 1 and the stderr is shown.</li>
</ul>


<p>(Note the ordering of <code>2&gt;&amp;1 &gt;/dev/null</code> to redirect stderr to stdout
and silence the original stdout, as opposed to the more common
incantation of <code>&gt;/dev/null 2&gt;&amp;1</code>, which silences both stderr and
stdout.)</p>

<p>The reason for this is that <code>myprog</code> has a bug that causes it to print
errors but not produce a proper exit status in some cases.</p>

<p>Now how will my little shell function snippet behave under zsh?  Well,
it&rsquo;s quite confusing at first, but the following happens.  If there is
stderr output, then only stderr is printed.  If there is no stderr
output, then stdout is passed through instead.  But that&rsquo;s not what I
wanted.</p>

<p>This can be reproduced simply:</p>

<pre><code>ls --bogus 2&gt;&amp;1 &gt;/dev/null | grep .
</code></pre>

<p>prints an error message, as expected, but</p>

<pre><code>ls 2&gt;&amp;1 &gt;/dev/null | grep .
</code></pre>

<p>prints a directory listing.  That&rsquo;s because zsh redirects stdout to
<em>both</em> <code>/dev/null</code> and the pipe, which makes the redirection to
<code>/dev/null</code> pointless.</p>

<p>Note that in bash, the second command prints nothing.</p>

<p>This behavior can be changed by turning off the <code>MULTIOS</code> option (see
<code>zshmisc</code> man page), and my first instinct was to do that, but options
are not lexically scoped (I think), so this would break again if the
option was somehow changed somewhere else.  Also, I think I kind of
like that option for interactive use.</p>

<p>My workaround is to use a subshell:</p>

<pre><code>! ( myprog "$arg" 2&gt;&amp;1 &gt;/dev/null ) | grep .
</code></pre>

<p>The long-term fix will probably be to write an external shell script
in bash or plain POSIX shell.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ccache and clang, part 3]]></title>
    <link href="http://peter.eisentraut.org/blog/2014/12/01/ccache-and-clang-part-3"/>
    <updated>2014-12-01T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2014/12/01/ccache-and-clang-part-3</id>
    <content type="html"><![CDATA[<p>In
<a href="http://petereisentraut.blogspot.com/2011/05/ccache-and-clang.html">part 1</a>
and
<a href="http://petereisentraut.blogspot.com/2011/09/ccache-and-clang-part-2.html">part 2</a>
I investigated how to use <code>ccache</code> with <code>clang</code>.  That was more than
three years ago.</p>

<p>I got an email the other day that
<a href="https://bugzilla.samba.org/show_bug.cgi?id=8118">ccache bug 8118</a>,
which I filed while writing part 1, was closed, as ccache 3.2 was
released.  The
<a href="https://ccache.samba.org/releasenotes.html#_ccache_3_2">release notes of ccache 3.2</a>
contain several items related to clang.  So it was time to give this
another look.</p>

<p>Basically, the conclusions from part 2 still stand: You cannot use
<code>ccache</code> with <code>clang</code> without using <code>CCACHE_CPP2</code>.  And it is now
becoming clear to me that this is an issue that is not going to go
away, and it&rsquo;s not really even Clang&rsquo;s fault.</p>

<h2>Warnings!</h2>

<p>The problem is that <code>clang</code>&rsquo;s <code>-Wall</code> can cause warnings when
compiling the <em>preprocessed</em> version of otherwise harmless C code.
This can be illustrated by this piece of C code:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="kt">int</span>
</span><span class='line'><span class="nf">foo</span><span class="p">()</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>        <span class="kt">int</span> <span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">q</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">p</span> <span class="o">=</span> <span class="n">q</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'>        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">;</span>
</span><span class='line'>        <span class="k">if</span> <span class="p">(</span><span class="n">p</span> <span class="o">==</span> <span class="n">p</span><span class="p">)</span>
</span><span class='line'>                <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span><span class='line'>        <span class="k">if</span> <span class="p">((</span><span class="n">p</span> <span class="o">==</span> <span class="n">q</span><span class="p">))</span>
</span><span class='line'>                <span class="k">return</span> <span class="mi">2</span><span class="p">;</span>
</span><span class='line'>        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>When compiled by <code>gcc-4.9 -Wall</code>, this gives no warnings.  When
compiled by <code>clang-3.5 -Wall</code>, this results in</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="n">test</span><span class="p">.</span><span class="nl">c</span><span class="p">:</span><span class="mi">7</span><span class="o">:</span><span class="mi">4</span><span class="o">:</span> <span class="nl">warning</span><span class="p">:</span> <span class="n">explicitly</span> <span class="n">assigning</span> <span class="n">value</span> <span class="n">of</span> <span class="n">variable</span> <span class="n">of</span> <span class="n">type</span> <span class="err">&#39;</span><span class="kt">int</span> <span class="o">*</span><span class="err">&#39;</span> <span class="n">to</span> <span class="n">itself</span> <span class="p">[</span><span class="o">-</span><span class="n">Wself</span><span class="o">-</span><span class="n">assign</span><span class="p">]</span>
</span><span class='line'><span class="n">test</span><span class="p">.</span><span class="nl">c</span><span class="p">:</span><span class="mi">8</span><span class="o">:</span><span class="mi">8</span><span class="o">:</span> <span class="nl">warning</span><span class="p">:</span> <span class="n">self</span><span class="o">-</span><span class="n">comparison</span> <span class="n">always</span> <span class="n">evaluates</span> <span class="n">to</span> <span class="nb">true</span> <span class="p">[</span><span class="o">-</span><span class="n">Wtautological</span><span class="o">-</span><span class="n">compare</span><span class="p">]</span>
</span><span class='line'><span class="n">test</span><span class="p">.</span><span class="nl">c</span><span class="p">:</span><span class="mi">10</span><span class="o">:</span><span class="mi">9</span><span class="o">:</span> <span class="nl">warning</span><span class="p">:</span> <span class="n">equality</span> <span class="n">comparison</span> <span class="n">with</span> <span class="n">extraneous</span> <span class="n">parentheses</span> <span class="p">[</span><span class="o">-</span><span class="n">Wparentheses</span><span class="o">-</span><span class="n">equality</span><span class="p">]</span>
</span><span class='line'><span class="n">test</span><span class="p">.</span><span class="nl">c</span><span class="p">:</span><span class="mi">10</span><span class="o">:</span><span class="mi">9</span><span class="o">:</span> <span class="nl">note</span><span class="p">:</span> <span class="n">remove</span> <span class="n">extraneous</span> <span class="n">parentheses</span> <span class="n">around</span> <span class="n">the</span> <span class="n">comparison</span> <span class="n">to</span> <span class="n">silence</span> <span class="n">this</span> <span class="n">warning</span>
</span><span class='line'><span class="n">test1</span><span class="p">.</span><span class="nl">c</span><span class="p">:</span><span class="mi">10</span><span class="o">:</span><span class="mi">9</span><span class="o">:</span> <span class="nl">note</span><span class="p">:</span> <span class="n">use</span> <span class="sc">&#39;=&#39;</span> <span class="n">to</span> <span class="n">turn</span> <span class="n">this</span> <span class="n">equality</span> <span class="n">comparison</span> <span class="n">into</span> <span class="n">an</span> <span class="n">assignment</span>
</span></code></pre></td></tr></table></div></figure>


<p>You wouldn&rsquo;t normally write code like this, but the C preprocessor
could create code with self-assignments, self-comparisons, extra
parentheses, and so on.</p>

<p>This example represents the issues I saw when trying to compile
PostgreSQL 9.4 with <code>ccache</code> and <code>clang</code>; there might be others.</p>

<p>You can address this issue in two ways:</p>

<ol>
<li><p>Use <code>CCACHE_CPP2</code>, as discussed in part 2.  With ccache 3.2, you
can now also put this into a configuration file: <code>run_second_cpp =
true</code> in <code>~/.ccache/ccache.conf</code></p></li>
<li><p>Turn off the warnings mentioned above: <code>-Wno-parentheses-equality</code>,
<code>-Wno-tautological-compare</code>, <code>-Wno-self-assign</code> (and any others you
might find).  One might think that these are actually useful warnings
that one might want to keep, but GCC doesn&rsquo;t warn about them, and if
you develop primarily with GCC, your code might contain these issues
anyway.  In particular, I have found that <code>-Wno-tautological-compare</code>
is necessary for legitimate code.</p></li>
</ol>


<p>I think <code>CCACHE_CPP2</code> is the way to go, for two reasons.  Firstly,
having to add more and more options to turn off warnings is obviously
somewhat stupid.  Secondly and more importantly, there is nothing
stopping GCC from adding warnings similar to Clang&rsquo;s that would
trigger on preprocessed versions of otherwise harmless C code.  Unless
they come up with a clever way to annotate the preprocessed code to
the effect of &ldquo;this code might look wrong to you, but it looked OK
before preprocessing, so don&rsquo;t warn about it&rdquo;, in a way that creates
<em>no</em> extra warnings and doesn&rsquo;t lose <em>any</em> warnings, I don&rsquo;t think
this issue can be solved.</p>

<h2>Speed!</h2>

<p>Now the question is, how much would globally setting <code>CCACHE_CPP2</code>
slow things down?</p>

<p>To test this, I have built PostgreSQL 9.4rc1 with <code>clang-3.5</code> and
<code>gcc-4.8</code> (not <code>gcc-4.9</code> because it creates some unrelated warnings
that I don&rsquo;t want to deal with here).  I have set <code>export
CCACHE_RECACHE=true</code> so that the cache is not read but new cache
entries are computed.  That way, the overhead of <code>ccache</code> on top of
the compiler is measured.</p>

<p>Results:</p>

<ul>
<li><code>clang-3.5</code>

<ul>
<li>Using <code>ccache</code> is 10% slower than not using it at all.</li>
<li>Using <code>ccache</code> with <code>CCACHE_CPP2</code> on is another 10% slower.</li>
</ul>
</li>
<li><code>gcc-4.8</code>

<ul>
<li>Using <code>ccache</code> is 19% slower than not using it at all.</li>
<li>Using <code>ccache</code> with <code>CCACHE_CPP2</code> is another 9% slower.</li>
</ul>
</li>
</ul>


<p>(There different percentages between <code>gcc</code> and <code>clang</code> arise because
<code>gcc</code> is faster than <code>clang</code> (yes, really, more on that in a future
post), but the overhead of <code>ccache</code> doesn&rsquo;t change.)</p>

<p>10% or so is not to be dismissed, but let&rsquo;s remember that this applies
only if there is a cache miss.  If everything is cached, both methods
do the same thing.  Also, if you use parallel make, the overhead is
divided by the number of parallel jobs.</p>

<p>With that in mind, I have decided to put the issue to rest for myself
and have made myself a <code>~/.ccache/ccache.conf</code> containing</p>

<pre><code>run_second_cpp = true
</code></pre>

<p>Now Clang or any other compiler should run without problems through
ccache.</p>

<h2>Color!</h2>

<p>There is one more piece of news in the new ccache release: Another
thing I talked about in part 1 was that <code>ccache</code> will disable the
colored output of <code>clang</code>, and I suggested workarounds.  This was
actually fixed in ccache 3.2, so the workarounds are no longer
necessary, and the above configuration change is really the only thing
to make Clang work smoothly with ccache.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Checking whitespace with Git]]></title>
    <link href="http://peter.eisentraut.org/blog/2014/11/04/checking-whitespace-with-git"/>
    <updated>2014-11-04T20:00:00-05:00</updated>
    <id>http://peter.eisentraut.org/blog/2014/11/04/checking-whitespace-with-git</id>
    <content type="html"><![CDATA[<p><a href="http://blog.codinghorror.com/whitespace-the-silent-killer/">Whitespace matters</a>.</p>

<p>Git has support for checking whitespace in patches.  <code>git apply</code> and <code>git am</code> have the option <code>--whitespace</code>, which can be used to warn or error about whitespace errors in the patches about to be applied. <code>git diff</code> has the option <code>--check</code> to check a change for whitespace errors.</p>

<p>But all this assumes that your existing code is cool, and only new changes are candidates for problems.  Curiously, it is a bit hard to use those same tools for going back and checking whether an existing tree satisfies the whitespace rules applied to new patches.</p>

<p>The core of the whitespace checking is in <code>git diff-tree</code>.  With the <code>--check</code> option, you can check the whitespace in the diff between two objects.</p>

<p>But how do you check the whitespace of a tree rather than a diff?  Basically, you want</p>

<pre><code>git diff-tree --check EMPTY HEAD
</code></pre>

<p>except there is no <code>EMPTY</code>.  But you can compute the hash of an empty Git tree:</p>

<pre><code>git hash-object -t tree /dev/null
</code></pre>

<p>So the full command is</p>

<pre><code>git diff-tree --check $(git hash-object -t tree /dev/null) HEAD
</code></pre>

<p>If have this as an alias in my <code>~/.gitconfig</code>:</p>

<pre><code>[alias]
    check-whitespace = !git diff-tree --check $(git hash-object -t tree /dev/null) HEAD
</code></pre>

<p>Then running</p>

<pre><code>git check-whitespace
</code></pre>

<p>can be as easy as running <code>make</code> or <code>git commit</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First post]]></title>
    <link href="http://peter.eisentraut.org/blog/2014/10/25/first-post"/>
    <updated>2014-10-25T14:47:42-04:00</updated>
    <id>http://peter.eisentraut.org/blog/2014/10/25/first-post</id>
    <content type="html"><![CDATA[<p>This is my new blog.  My old blog was <a href="http://petereisentraut.blogspot.com/">here</a>, but it was time to move on.</p>
]]></content>
  </entry>
  
</feed>
